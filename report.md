### Ruta paso a paso para tu **pipeline de an√°lisis, reglas de asociaci√≥n y clustering**

> **Supuestos de hardware**
> ‚Ä¢ Laptop/PC con 8‚Äì12 GB RAM y 4‚Äì8 n√∫cleos CPU
> ‚Ä¢ Sin GPU: los tiempos que doy son *orientativos* (¬±30 %).
> ‚Ä¢ Trabajar√°s dentro de un entorno `venv` con *pandas*, *numpy*, *scikit-learn*, *mlxtend*, *scipy*, *seaborn/plotly*.

---

## 1 ¬∑ Ingesta y organizaci√≥n de datos

| Paso                                                | Qu√© haces                                                                              | D√≥nde queda el c√≥digo      | Tiempo de **dev** | Tiempo **runtime** |
| --------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------- | ----------------- | ------------------ |
| 1.1 Cargar `challenge_set.json`                     | Iterar playlists, almacenar `pid`, `name`, `tracks` en listas y normalizar a DataFrame | `src/01_load_playlists.py` | 1 h               | < 5 min            |
| 1.2 Filtrar playlists vac√≠as                        | `df_playlists = df_playlists[df_playlists.num_tracks>0]`                               | ‚Äî                          | 10 min            | instante           |
| 1.3 Cargar `acousticbrainz_data_updated_clean.json` | Crear dict ‚Üí DataFrame (una fila/track)                                                | `src/02_load_acoustic.py`  | 1 h               | < 10 min           |
| 1.4 ‚ÄúExpandir‚Äù playlists a nivel track              | `playlist_tracks = df_playlists.explode('tracks')` ‚Üí normalizar columnas track         | `src/03_flatten.py`        | 1 h               | 3‚Äì5 min            |
| 1.5 Integrar metadata + audio features              | `pd.merge` por `track_uri`                                                             | ‚Äî                          | 30 min            | < 5 min            |

> üîë **Resultado**:
> ‚Ä¢ `tracks_df.csv` (‚âà 66 k filas √ó \~150 columnas)
> ‚Ä¢ `playlists_df.csv` (‚âà 10 k playlists)
> ‚Ä¢ `playlist_track_df.csv` (‚âà 650 k filas)

---

## 2 ¬∑ An√°lisis exploratorio (EDA)

1. **Distribuci√≥n de longitud de playlists**

   ```python
   playlist_track_df.groupby('pid').size().hist()
   ```

   * Media \~65, m√°x 100, min 1.

2. **An√°lisis de g√©neros / moods**

   * Contar `highlevel.genre_dortmund.value`, `mood_party.value`, etc.
   * *Pareto* de los 10 g√©neros m√°s frecuentes.

3. **Correlaciones num√©ricas** (`bpm`, `energy`, `danceability_ll`, `loudness`, ‚Ä¶).

   * Heatmap Pearson / Spearman.
   * Detectar pares muy colineales para PCA.

4. **Outliers num√©ricos**

   * Boxplots por variable.
   * Z-score > 3 o IQR‚ÄÜ√ó‚ÄÜ1.5 ‚Üí marcar.

5. **Valores faltantes**

   * Tabla `% missing` por columna.
   * Variables con > 20 % vac√≠os: eliminar o imputar con media/mediana (dependiendo).

> ‚è±Ô∏è **Tiempos**: 4‚Äì6 h (explorar + graficar) ¬∑ ejecuci√≥n de celdas << 5 min.

---

## 3 ¬∑ Preprocesamiento

| Tarea                         | Herramienta / t√©cnica                                                   | Notas                               | Dev    | Run           |
| ----------------------------- | ----------------------------------------------------------------------- | ----------------------------------- | ------ | ------------- |
| Eliminar duplicados de tracks | `tracks_df.drop_duplicates('track_uri')`                                | ‚Äî                                   | 5 min  | instante      |
| Normalizar num√©ricos          | `StandardScaler` o `MinMaxScaler`                                       | Guardar *scaler* con `joblib`       | 15 min | 1 min         |
| Tratamiento de NA             | `SimpleImputer` media/mediana; o drop columnas con > 30 % NA            | ‚Äî                                   | 20 min | 1 min         |
| Outliers                      | Winsorizar o recortar                                                   | Opcional para clustering            | 30 min | 1 min         |
| Discretizar para asociaci√≥n   | `KBinsDiscretizer` o percentiles para `bpm`, `energy` (alto/medio/bajo) | Genera columnas tipo `bpm_high`=1/0 | 45 min | 2 min         |
| Binarizar pista-playlist      | `MultiLabelBinarizer` sobre `track_uri` √≥ sobre *tags*                  | Sparse matrix 10 k √ó 66 k           | 45 min | \~10 min      |
| Reducci√≥n de dimensionalidad  | PCA (k=50) para num√©ricos; UMAP/t-SNE 2 D para visual                   | Para graficar clusters              | 1 h    | UMAP 5‚Äì10 min |

> ‚è±Ô∏è **Bloque completo**: 4‚Äì6 h dev ¬∑ 15‚Äì30 min runtime.

---

## 4 ¬∑ Reglas de asociaci√≥n

### 4.1 Preparar transacciones

* **Opci√≥n 1 (cl√°sica):** cada playlist = conjunto de `track_uri`.
* **Opci√≥n 2 (sem√°ntica):** agrupa por `genre_highlevel` o `mood_party` ‚Üí transacciones m√°s cortas.

### 4.2 Algoritmos

| Algoritmo             | Librer√≠a                             | Soporte inicial         | Tiempo *(10 k tx)* |
| --------------------- | ------------------------------------ | ----------------------- | ------------------ |
| **Apriori** (Agrawal) | `mlxtend.frequent_patterns.apriori`  | `min_support` 0.01‚Äì0.02 | 2‚Äì8 min            |
| **FP-Growth**         | `mlxtend.frequent_patterns.fpgrowth` | `min_support` igual     | 1‚Äì5 min            |

> Conjuntos frecuentes ‚Üí `association_rules()` (confianza, lift).
> Ordena por *lift* > 1 y muestra top 20.

### 4.3 Entregables

* Tabla CSV ‚Äúreglas\_significativas.csv‚Äù.
* Gr√°fico red de reglas (source ‚Üí target, grosor=lift).
* Discusi√≥n: *‚ÄúLas playlists ‚ÄòParty‚Äô presentan co-ocurrencia alta entre tracks con `genre_dortmund=electronic` y `mood_party=party` (lift = 3.2)‚Ä¶‚Äù*.

> ‚è±Ô∏è **Dev** 2‚Äì3 h ¬∑ **Ejecuci√≥n** 5‚Äì15 min (dependiendo soporte).

---

## 5 ¬∑ Clustering (agrupaci√≥n)

### 5.1 Features de entrada

| Nivel        | Vector                                                                      | Dimensi√≥n t√≠pica |
| ------------ | --------------------------------------------------------------------------- | ---------------- |
| **Track**    | `[bpm, energy, danceability_ll, loudness, ‚Ä¶]`                               | 10‚Äì30            |
| **Playlist** | Promedio/mediana de features de sus canciones + proporci√≥n de g√©neros/moods | 30‚Äì60            |

### 5.2 Algoritmos m√≠nimos (3)

| Algoritmo         | `sklearn` clase           | Hiperpar√°metros                        | Ventajas                             |
| ----------------- | ------------------------- | -------------------------------------- | ------------------------------------ |
| **K-Means**       | `KMeans`                  | `k` = 8‚Äì15 (usar *elbow* + Silhouette) | R√°pido, baseline                     |
| **DBSCAN**        | `DBSCAN`                  | `eps`, `min_samples` (tune con k-dist) | Detecta densidades; identifica ruido |
| **Agglomerative** | `AgglomerativeClustering` | `linkage='ward'`, `n_clusters` = k     | Visualizable con dendrograma         |

*(Opcional bonus: Gaussian Mixture o HDBSCAN).*

### 5.3 Evaluaci√≥n

* **Internas:** Silhouette Score, Davies-Bouldin.
* **Externas:** si tuvieras etiqueta ‚Äúg√©nero dominante‚Äù podr√≠as usar *Adjusted Rand*.
* Tabla comparativa.

### 5.4 Visualizaci√≥n

* Reducir a 2 D con UMAP ‚Üí color por cluster.
* Gr√°fica radar por cluster con medias de `energy`, `danceability_ll`, etc.

> ‚è±Ô∏è **Dev** 3‚Äì4 h ¬∑ **Train + m√©tricas** 5‚Äì20 min (UMAP \~10 min).

---

## 6 ¬∑ Informe final y discusi√≥n

1. **Introducci√≥n** (objetivo, dataset).
2. **EDA** (hallazgos clave, gr√°ficas).
3. **Preprocesamiento** (qu√© hiciste y por qu√©).
4. **Reglas de asociaci√≥n** (metodolog√≠a, top reglas, aplicaci√≥n pr√°ctica: recomendar canciones).
5. **Clustering** (m√©todos, par√°metros, comparativa, interpretaci√≥n de clusters).
6. **Conclusiones** (limitaciones, trabajo futuro).

> ‚è±Ô∏è Redacci√≥n + figuras: 1 d√≠a.

---

## Cronograma sugerido (en d√≠as de trabajo intensivo)

| D√≠a   | Horas estimadas | Entregable                                      |
| ----- | --------------- | ----------------------------------------------- |
| **1** | 6‚Äì8 h           | Carga de datos + EDA preliminar                 |
| **2** | 6‚Äì7 h           | Preprocesamiento completo                       |
| **3** | 5 h             | Reglas de asociaci√≥n + discusi√≥n                |
| **4** | 6 h             | Feature playlist-level + clustering             |
| **5** | 6 h             | Comparativas, visualizaciones, pulir resultados |
| **6** | 8 h             | Redactar informe y preparar slides/notebook     |

Total ‚âà **37‚Äì40 h** (una semana laboral). Ajusta seg√∫n tu familiaridad con *pandas* y *scikit-learn*.

---

### Consejos pr√°cticos

* **Trabaja en notebooks Jupyter** para EDA y prototipos; pasa lo estable a scripts en `src/`.
* **Guarda artefactos intermedios** (`.pkl` de DataFrames, scaler, modelos) para no recalcular.
* Usa **`pd.read_json(..., lines=True)`** si conviertes los JSON a *jsonl*; consume menos RAM.
* Para la **matriz playlist-track**, usa `scipy.sparse.csr_matrix` para que Apriori/FP-Growth no reviente memoria.
* Fija semillas (`random_state`) y documenta versiones (`pip freeze > requirements_locked.txt`).

¬°Con esto tienes un plan claro, tiempos estimados y el mapa de c√≥digo que necesitas para completar tu proyecto de miner√≠a de datos de principio a fin!
